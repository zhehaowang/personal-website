\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}

\setlength\parindent{0pt}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*\mean[1]{\bar{#1}}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}    

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\mmax}{max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\title{Defitions and notations}
\author{Zhehao Wang}

\maketitle{}

\subsection{Limit}

Let $f(x)$ be a function defined on an interval that contains $x = a$, except possibly at $x = a$, then we say that
$$
\lim\limits_{x \to a}{f(x) = L}
$$
if for every $\epsilon > 0$ there is some number $\delta > 0$ such that
$$
|f(x) - L| < \epsilon ~ \text{whenever} ~ 0 < |x - a| < \delta
$$

\subsection{Gradient}
Given $f(x_1, x_n)$ on $\mathbf{R}^n$
$$
\bigtriangledown f(a_1, ..., a_n) = (\frac{\partial}{\partial x_1}(a_1, ..., a_n), ..., \frac{\partial}{\partial x_n}(a_1, ..., a_n))
$$

\subsection{Directional derivative}

\textbf{Homework notation}
$$
f'(x; u) = \lim\limits_{h \to 0}{\frac{f(x + hu) - f(x)}{h}}
$$

\textbf{Wikipedias notation}
note that it's $f$ on a vector $\vec{x}$ having its derivative calculated on direction $\vec{v}$, essentially gradient on direction $\vec{v}$.
$$
\bigtriangledown_v f(\vec{x}) = \lim\limits_{h \to 0}{\frac{f(\vec{x} + h\vec{v}) - f(\vec{x})}{h}}
$$

\subsection{Vector norm}
A norm is a function that assigns a stricly positive length or size to each vector in a vector space (except the zero vector which is assigned a length of 0).

\textbf{Absolute value norm} is a norm on the one-dimensional vector spaces formed by real or complex numbers.
$$
\norm{x} = |x|
$$

\textbf{Euclidean norm} on a Euclidean space $\mathbf{R}^n$ is such
$$
\norm{\vec{x}}_2 = \sqrt{x_1^2 + ... + x_n^2}
$$

\textbf{Manhattan or taxicab norm}
$$
\norm{\vec{x}}_1 = \sum_{i = 1}^{n}{|x_i|}
$$

\textbf{$p$-norm}
$$
\norm{\vec{x}}_p = (\sum_{i = 1}^{n}{|x_i|^p})^{\frac{1}{p}}
$$
Note that when $p = 1$, we get Manhattan norm, and when $p = 2$, we get Euclidean norm.

When $p = \infty$
$$
\norm{\vec{x}}_{\infty} = {\mmax_{i}{|x_i|}}
$$

\subsection{argmax}

Points of the domain of some function at which the function values are maximized.

Given an arbitrary set $X$, a totally ordered set $Y$ and a function $f: X \to Y$, the $\argmax$ over some subset $S$ of $X$ is defined by
$$
\argmax_{x \in S \subseteq X}{f(x)} = \{x ~ | ~ x \in S \land \forall y \in S : f(y) \leq f(x)\}
$$

\subsection{Random variable}

A random variable $X : \Omega \to \mathit{E}$ is a measurable function from a set of possible outcomes $\Omega$ to a measurable space $\mathit{E}$.
Often times $\mathit{E} = \mathbb{R}$

The probability that $X$ takes on a value in a measurable set $S \subseteq \mathit{E}$ is written as
$$
Pr(X \in S) = P({\omega \in \Omega | X(\omega) \in S})
$$

\textbf{Intuition}: mapping outcomes of a random process to numbers, like this definition of $X$
$$
X =
\begin{cases}
0, & \text{if heads} \\
1, & \text{if tails}
\end{cases}
$$
Instead of a traditional algebraic variable that can be solved for one value, a random variable can have different values (each with a probability) under different conditions.

\subsection{Law of large numbers}

$X_i, X_2, ...$ is an infinite sequence of independent and identically distributed (iid) random variables with expected value $E(X_1) = E(X_2) = ... = \mu$, and
$$
\mean{X}_n = \frac{1}{n}(X_1 + ... + X_n) ~ \footnote{What does the sum of random variables mean? Seems that each $X_i$ here means 'sampled' values following the distribution defined by the random variable, like, the outcome of an experiment}
$$


\textbf{The weak law} states that for any positive number $\epsilon$
$$
\lim\limits_{n \to \infty}{Pr(|\mean{X}_n - \mu| > \epsilon)} = 0
$$

\textbf{The strong law} states that
$$
Pr(\lim\limits_{n \to \infty}{\mean{X}_n = \mu}) = 1
$$

\textbf{Intuition}: law of large numbers is a theorem that describes the average of the results obtained from a large number of trials should be close to the expected value, and will tend to become closer as more trials are performed.

\end{document}