\documentclass{article}

\usepackage{amsmath}
\usepackage{amssymb}

\setlength\parindent{0pt}

\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand*\mean[1]{\bar{#1}}

\renewcommand{\thefootnote}{\fnsymbol{footnote}}    

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\mmax}{max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\title{Definitions and notations}
\author{Zhehao Wang}

\maketitle{}

\section{Calculus and linear algebra}

\subsection{Limit}

Let $f(x)$ be a function defined on an interval that contains $x = a$, except possibly at $x = a$, then we say that
$$
\lim\limits_{x \to a}{f(x) = L}
$$
if for every $\epsilon > 0$ there is some number $\delta > 0$ such that
$$
|f(x) - L| < \epsilon ~ \text{whenever} ~ 0 < |x - a| < \delta
$$

\subsection{Gradient}
Given $f(\vec{x})$ where $\vec{x} = (x_1, ..., x_n)$ on $\mathbb{R}^n$
$$
\bigtriangledown f(a_1, ..., a_n) = (\frac{\partial f}{\partial x_1}(a_1, ..., a_n), ..., \frac{\partial f}{\partial x_n}(a_1, ..., a_n))
$$

\textbf{Intuition}: gradient is a vector (the rate of change of your function, when you move in a certain direction), which in a two-dimensional space, tangents the curve at a given point.

\subsection{Directional derivative}

\textbf{Homework notation}
$$
f'(x; u) = \lim\limits_{h \to 0}{\frac{f(x + hu) - f(x)}{h}}
$$

\textbf{Wikipedias notation}

$$
\bigtriangledown_v f(\vec{x}) = \lim\limits_{h \to 0}{\frac{f(\vec{x} + h\vec{v}) - f(\vec{x})}{h}}
$$

\textbf{Intuition}: the rate-of-change of a function $f(\vec{x})$ on direction $\vec{v}$.

\subsection{Vector norm}
A norm is a function that assigns a stricly positive length or size to each vector in a vector space (except the zero vector which is assigned a length of 0).

\textbf{Absolute value norm} is a norm on the one-dimensional vector spaces formed by real or complex numbers.
$$
\norm{x} = |x|
$$

\textbf{Euclidean norm} on a Euclidean space $\mathbb{R}^n$ is such
$$
\norm{\vec{x}}_2 = \sqrt{x_1^2 + ... + x_n^2}
$$

\textbf{Manhattan or taxicab norm}
$$
\norm{\vec{x}}_1 = \sum_{i = 1}^{n}{|x_i|}
$$

\textbf{$p$-norm}
$$
\norm{\vec{x}}_p = (\sum_{i = 1}^{n}{|x_i|^p})^{\frac{1}{p}}
$$
Note that when $p = 1$, we get Manhattan norm, and when $p = 2$, we get Euclidean norm.

When $p = \infty$
$$
\norm{\vec{x}}_{\infty} = {\mmax_{i}{|x_i|}}
$$

\subsection{argmax}

Points of the domain of some function at which the function values are maximized.

Given an arbitrary set $X$, a totally ordered set $Y$ and a function $f: X \to Y$, the $\argmax$ over some subset $S$ of $X$ is defined by
$$
\argmax_{x \in S \subseteq X}{f(x)} = \{x ~ | ~ x \in S \land \forall y \in S : f(y) \leq f(x)\}
$$

\subsection{Jacobian matrix}

\textbf{Jacobian matrix} is the matrix of all first-order partial derivatives of a vector-valued function.

Suppose $\vec{f} : \mathbb{R}^n \to \mathbb{R}^m$, the Jacobian matrix $\vec{J}$ of $\vec{f}$ is defined as follows

$$
\vec{J} = \begin{bmatrix} ~ \frac{\partial \vec{f}}{\partial x_1} ~ ... ~ \frac{\partial \vec{f}}{\partial x_n} ~ \end{bmatrix}
$$

or component-wise $\vec{J}_{ij} = \frac{\partial \vec{f_i}}{\partial x_j}$, meaning

$$
\vec{J} = 
 \begin{bmatrix}
  \frac{\partial \vec{f_1}}{\partial x_1} ~ ... ~ \frac{\partial \vec{f_1}}{\partial x_n} \\
  ... \\
  \frac{\partial \vec{f_m}}{\partial x_1} ~ ... ~ \frac{\partial \vec{f_m}}{\partial x_n}
 \end{bmatrix}
$$

\section{Probability}

\subsection{Random variable}

A random variable $X : \Omega \to \mathit{E}$ is a measurable function from a set of possible outcomes $\Omega$ to a measurable space $\mathit{E}$.
Often times $\mathit{E} = \mathbb{R}$

The probability that $X$ takes on a value in a measurable set $S \subseteq \mathit{E}$ is written as
$$
Pr(X \in S) = P({\omega \in \Omega | X(\omega) \in S})
$$

\textbf{Intuition}: mapping outcomes of a random process to numbers, like this definition of $X$
$$
X =
\begin{cases}
0, & \text{if heads} \\
1, & \text{if tails}
\end{cases}
$$
Instead of a traditional algebraic variable that can be solved for one value, a random variable can have different values (each with a probability) under different conditions.

\subsection{Law of large numbers}
\label{law-of-large-numbers}

$X_i, X_2, ...$ is an infinite sequence of independent and identically distributed (iid) random variables with expected value $E(X_1) = E(X_2) = ... = \mu$, and
$$
\mean{X}_n = \frac{1}{n}(X_1 + ... + X_n) ~ \footnote{What does the sum of random variables mean? Seems that each $X_i$ here means 'sampled' values following the distribution defined by the random variable, like, the outcome of an experiment}
$$


\textbf{The weak law} states that for any positive number $\epsilon$
$$
\lim\limits_{n \to \infty}{Pr(|\mean{X}_n - \mu| > \epsilon)} = 0
$$

\textbf{The strong law} states that
$$
Pr(\lim\limits_{n \to \infty}{\mean{X}_n = \mu}) = 1
$$

\textbf{Intuition}: law of large numbers is a theorem that describes the average of the results obtained from a large number of trials should be close to the expected value, and will tend to become closer as more trials are performed.


\section{Statistical learning}

Let $\mathbf{X}$ denote the input space, $\mathbf{Y}$ denote the output space, and $\mathbf{A}$ denote the action space.

A \textbf{decision function / prediction function} $f: \mathbf{X} \to \mathbf{A}$ maps an input to an action.

A \textbf{loss function} $l: \mathbf{A} \times \mathbf{Y} \to \mathbb{R}$ evaluates an action in the context of an output, and maps the error to a real.
In traditional problems we can usually assume action has no effect on the output (like stock market prediction).

\subsection{Risk, Bayesian function, empirical risk}

Assume there is a data generating distribution $P_{\mathbf{X} \times \mathbf{Y}}$.

\textbf{Risk} of a decision function can then be defined as
$$
R(f) = \mathbb{E}l(f(x), y)
$$
Where an input/output pair $(x, y)$ is generated independently and identically distributed from $P_{\mathbf{X} \times \mathbf{Y}}$.

A \textbf{Bayesian decision function / target function} $f^* : \mathbf{X} \to \mathbf{A}$ is a function that achieves the minimal risk among all possible functions.
$$
f^* = \argmin_{f}{R(f)}
$$

Risk cannot be calculated as we are not given $P_{\mathbf{X} \times \mathbf{Y}}$.

Let $\mathbf{D}_n = \{(x_1, y_1), ..., (x_n, y_n)\}$ be drawn iid from $P_{\mathbf{X} \times \mathbf{Y}}$.

The \textbf{empirical risk} of $f : \mathbf{X} \to \mathbf{A}$ with respect to $\mathbf{D}_n$ is

$$
\hat{R}_n(f) = \frac{1}{n} \sum_{i = 1}^{n}{l(f(x_i), y_i)}
$$

By strong law of large numbers (\ref{law-of-large-numbers}), when $n$ is big enough, empirical risk can be used to approximate risk.

\subsection{Empirical risk minimization, constrained empirical risk minimization}

A function $\hat{f}$ is an \textbf{empirical risk minimizer} if
$$
\hat{f} = \argmin_{f}{\hat{R}_n(f)}
$$
The prediction function that produces the smallest empiricall risk over set $\mathbf{D}_n$.
This naturally leads to overfit.

So we minimize empirical risk, but only within a hypothesis space $\mathbf{F}$, being a set of prediction functions.

\textbf{Constrained empirical risk minimizer} can then be defined as
$$
\hat{f}_n = \argmin_{f \in \mathbf{F}}{\frac{1}{n} \sum_{i = 1}^{n}{l(f(x_i), y_i)}}
$$

If we plug Risk in instead, a \textbf{constrained risk minimizer} becomes
$$
f^*_{\mathbf{F}} = \argmin_{f \in \mathbf{F}}{\mathbb{E}l(f(x), y)}
$$

\textbf{Approximation error} is the risk difference between the constrained risk minimizer (in $\mathbf{F}$) and the target

$$
R(f_{\mathbf{F}}) - R(f^*)
$$

\textbf{Estimation error} is the risk difference between the constrained empirical risk minimizer and the constrained risk minimizer (both in $\mathbf{F}$)

$$
R(\hat{f}_n) - R(f_{\mathbf{F}})
$$

\textbf{Optimization error} is the risk difference between a function $\tilde{f}_n$ (which we find in practice) and the constrained empirical risk minimizer
\footnote{Optimization can be negative, however, this $\hat{R}(\tilde{f}_n) - \hat{R}(\hat{f}_n)$ can't.
In this hypothesis space, this function can fit the overall world better than the constrained empirical risk minimizer, but it can't on the training set, on which the constrained ERM performs best in this space}

$$
R(\tilde{f}_n) - R(\hat{f}_n)
$$

\textbf{Excess risk} is the risk between a function and the target
$$
\text{Excess Risk}(\hat{f}_n) = \text{Estimation Error} + \text{Approximation Error} = R(\hat{f}_n) - R(f^*)
$$
$$
\text{Excess Risk}(\tilde{f}_n) = \text{Optimization Error} + \text{Estimation Error} + \text{Approximation Error}
    = R(\tilde{f}_n) - R(f^*)
$$

\subsection{Recap: linear regression with square loss}


\end{document}
