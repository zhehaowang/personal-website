{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics\n",
    "\n",
    "A **statistic** is something you calculate on a sequence of r.v. $X_1, \\dots X_n$.\n",
    "\n",
    "**Degree of freedom** is the number of variables in the calculation of a statistic that are free to vary.\n",
    "\n",
    "A **statistical experiment** is a pair (Event Space, Probability distribution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99.59653741897253, 92.581884783705, 106.61119005424007)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: given data (normal distribution) and desired level of confidence,\n",
    "# calculate the confidence interval of mean\n",
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    n = len(data)\n",
    "    # mean and std error of mean\n",
    "    m, se = np.mean(data), stats.sem(data)\n",
    "    # PPF, percent-point-function inverse of the Gaussian CDF\n",
    "    h = se * stats.t.ppf((1 + confidence) / 2., n - 1)\n",
    "    return m, m - h, m + h\n",
    "\n",
    "mu = 100\n",
    "sigma = 10\n",
    "samples = 10\n",
    "data = np.random.normal(mu, sigma, samples)\n",
    "mean_confidence_interval(data, 0.95)\n",
    "# With 95% confidence, population mean fall into this range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametric hypothesis testing\n",
    "\n",
    "Type I error is where you erroneously rejected null hypothesis.\n",
    "\n",
    "Type II error is where you failed to reject null hypothesis whereas you should have.\n",
    "\n",
    "A **test** is a statistic that maps sample data to 0 or 1, 0 - reject null hypothesis, 1 - don't reject null hypothesis.\n",
    "\n",
    "**Level of the test** (defined on $\\Theta$) is when we shouldn't reject null hypothesis ($\\theta \\in \\Theta_0$), the maximum likelihood that we do (erroneously) reject it.\n",
    "\n",
    "**Power of the test** (defined on $\\Theta$) is when we should reject null hypothesis ($\\theta \\in \\Theta_1$), the minimum likelihood of us (correctly) rejecting null hypothesis.\n",
    "\n",
    "**p-value of the test** is the smallest level at which the test rejects the null hypothesis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.858925033055968"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# T-test\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "## Define 2 random distributions\n",
    "# Sample Size\n",
    "N = 10\n",
    "# Gaussian distributed data with mean = 2 and var = 1\n",
    "a = np.random.randn(N) + 2\n",
    "# Gaussian distributed data with with mean = 0 and var = 1\n",
    "b = np.random.randn(N)\n",
    "\n",
    "## Calculate the Standard Deviation\n",
    "# Calculate the variance to get the standard deviation\n",
    "\n",
    "# For unbiased max likelihood estimate we have to divide the var by N-1, and therefore the parameter ddof = 1\n",
    "var_a = a.var(ddof=1)\n",
    "var_b = b.var(ddof=1)\n",
    "\n",
    "# std deviation\n",
    "s = np.sqrt((var_a + var_b)/2)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t = 5.138377992786296\n",
      "p = 6.88798215611186e-05\n",
      "t = 5.138377992786296\n",
      "p = 6.887982156104175e-05\n"
     ]
    }
   ],
   "source": [
    "## Calculate the t-statistics\n",
    "t = (a.mean() - b.mean()) / (s * np.sqrt(2 / N))\n",
    "\n",
    "## Compare with the critical t-value\n",
    "# Degrees of freedom\n",
    "df = 2 * N - 2\n",
    "\n",
    "# p-value after comparison with the t \n",
    "p = 1 - stats.t.cdf(t, df = df)\n",
    "\n",
    "\n",
    "print(\"t = \" + str(t))\n",
    "print(\"p = \" + str(2*p))\n",
    "### You can see that after comparing the t statistic with the critical t value (computed internally) we get a good p value of 0.0005 and thus we reject the null hypothesis and thus it proves that the mean of the two distributions are different and statistically significant.\n",
    "\n",
    "## Cross Checking with the internal scipy function\n",
    "t2, p2 = stats.ttest_ind(a, b)\n",
    "print(\"t = \" + str(t2))\n",
    "print(\"p = \" + str(p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=0.08471721030905599, pvalue=0.9334213592888135)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = np.random.randn(N)\n",
    "b1 = np.random.randn(N)\n",
    "\n",
    "stats.ttest_ind(a1, b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation testing\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
